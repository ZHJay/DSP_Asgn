# -*- coding: utf-8 -*-
"""
Browser-based chat UI built on top of zero-raw.py utilities with LOCAL LLM.

Pipeline per turn:
1. User records audio in the browser (Gradio microphone) or uploads audio file.
2. Server transcribes speech via Whisper.
3. Text is sent to LOCAL LLM running on 127.0.0.1:1234 (fast LLM response).
4. Reply text is synthesized by our XTTS-based TTS.
5. Browser shows chat history and plays the generated wav.
"""

from __future__ import annotations

import os
import time
from pathlib import Path
from typing import Dict, List, Tuple

import gradio as gr
import librosa
import numpy as np
import requests
import soundfile as sf
import torch
import whisper

import zero_raw  # reuse STT/TTS classes
from shared_layer import Config as SharedLayerConfig

# torch.load pickles reference __main__.Config; expose alias so checkpoints work
zero_raw.Config = SharedLayerConfig


# -----------------------------------------------------------------------------
# Globals / configs
# -----------------------------------------------------------------------------

LOCAL_LLM_URL = os.getenv("LOCAL_LLM_URL", "http://localhost:1234/v1/chat/completions")
WHISPER_MODEL_NAME = os.getenv("WHISPER_MODEL", "small")
WHISPER_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
OUTPUT_DIR = Path(zero_raw.tts_config.output_dir)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

whisper_model = whisper.load_model(WHISPER_MODEL_NAME, device=WHISPER_DEVICE)
tts_system = zero_raw.UnifiedTTSSystem(tts_model="xtts")


# -----------------------------------------------------------------------------
# Local LLM client
# -----------------------------------------------------------------------------

def local_llm_chat(messages: List[Dict[str, str]]) -> str:
    """
    Call local LLM running on 127.0.0.1:1234
    Compatible with OpenAI API format (e.g., LM Studio, llama.cpp server, etc.)
    """
    headers = {
        "Content-Type": "application/json",
    }
    payload = {
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": -1,
        "stream": False
    }
    
    try:
        resp = requests.post(
            LOCAL_LLM_URL,
            headers=headers,
            json=payload,
            timeout=120  # æœ¬åœ°æ¨¡å‹å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
        )
        resp.raise_for_status()
        return resp.json()["choices"][0]["message"]["content"].strip()
    except requests.exceptions.ConnectionError:
        raise Exception("æ— æ³•è¿æ¥åˆ°æœ¬åœ°LLMæœåŠ¡ (localhost:1234)ã€‚è¯·ç¡®ä¿æœ¬åœ°æ¨¡å‹æœåŠ¡å·²å¯åŠ¨ã€‚")
    except requests.exceptions.Timeout:
        raise Exception("æœ¬åœ°LLMå“åº”è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¿è¡ŒçŠ¶æ€ã€‚")
    except Exception as e:
        raise Exception(f"æœ¬åœ°LLMè°ƒç”¨å¤±è´¥: {str(e)}")


# -----------------------------------------------------------------------------
# STT helper
# -----------------------------------------------------------------------------

def transcribe(audio_path: str, language: str) -> str:
    lang = "zh" if language.lower().startswith("zh") else None
    result = whisper_model.transcribe(audio_path, language=lang)
    return result.get("text", "").strip()


# -----------------------------------------------------------------------------
# Conversation states
# -----------------------------------------------------------------------------

SYSTEM_PROMPT = "You are a friendly conversational assistant."


def chat_pipeline(
    mic_audio: Tuple[int, np.ndarray],
    file_audio: str,
    reference_voice: str,
    language: str,
    history: List[Tuple[str, str]],
    messages_state: List[Dict[str, str]],
):
    if mic_audio is None and file_audio is None:
        return history, messages_state, None, "è¯·å…ˆå½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶å†æäº¤ã€‚"
    if not reference_voice:
        return history, messages_state, None, "è¯·ä¸Šä¼ ç›®æ ‡éŸ³è‰²ï¼ˆå‚è€ƒéŸ³é¢‘ï¼‰ã€‚"

    # Use file audio if provided, otherwise use microphone recording
    if file_audio is not None:
        temp_path = file_audio
    else:
        # Save microphone recording (gradio returns (sr, data))
        sr, data = mic_audio
        temp_path = OUTPUT_DIR / f"user_{int(time.time())}.wav"
        sf.write(temp_path, data, sr)

    user_text = transcribe(str(temp_path), language)
    if not user_text:
        user_text = "[æœªè¯†åˆ«è¯­éŸ³]"

    if not messages_state:
        messages_state = [{"role": "system", "content": SYSTEM_PROMPT}]
    messages_state.append({"role": "user", "content": user_text})
    history.append((user_text, "â€¦"))

    try:
        assistant_text = local_llm_chat(messages_state)
    except Exception as exc:
        history[-1] = (user_text, f"[æœ¬åœ°LLMè°ƒç”¨å¤±è´¥: {exc}]")
        return history, messages_state, None, f"æœ¬åœ°LLMè°ƒç”¨å¤±è´¥: {exc}"

    messages_state.append({"role": "assistant", "content": assistant_text})
    history[-1] = (user_text, assistant_text)

    out_path = OUTPUT_DIR / f"digital_reply_{int(time.time())}.wav"
    try:
        tts_system.synthesize(assistant_text, reference_voice, str(out_path), language)
    except Exception as exc:
        history[-1] = (user_text, f"[TTS å¤±è´¥: {exc}]")
        return history, messages_state, None, f"TTS å¤±è´¥: {exc}"

    return history, messages_state, str(out_path), "å®Œæˆ"


# -----------------------------------------------------------------------------
# UI
# -----------------------------------------------------------------------------

CUSTOM_CSS = """
body {
    background: #edf1ff;
    color: #0f172a;
    font-family: "Inter", "PingFang SC", sans-serif;
}
.gradio-container {
    max-width: 1200px !important;
    margin: 0 auto !important;
    padding: 30px 0 60px;
}
.hero-card {
    background: linear-gradient(135deg, #0f152b 0%, #1a2450 100%);
    border-radius: 22px;
    padding: 32px 42px;
    box-shadow: 0 25px 45px rgba(15, 23, 42, 0.35);
    margin-bottom: 26px;
    color: #ffffff;
}
.hero-card .markdown {
    color: #ffffff;
}
.the-elite {
    font-size: 13px;
    letter-spacing: 0.32em;
    color: #b3e5ff;
    text-transform: uppercase;
}
.hero-title {
    font-size: 34px;
    font-weight: 640;
    margin: 10px 0;
    color: #ffffff;
}
.hero-subtitle {
    color: #f1f5ff;
    line-height: 1.6;
}
.main-row {
    display: grid;
    grid-template-columns: 0.42fr 0.58fr;
    gap: 24px;
    align-items: start;
}
.panel {
    background: #ffffff;
    border-radius: 20px;
    padding: 24px;
    border: 1px solid #e2e8f0;
    box-shadow: 0 18px 35px rgba(15, 23, 42, 0.12);
    min-height: 520px;
    display: flex;
    flex-direction: column;
}
.panel h3 {
    margin-bottom: 16px;
    font-size: 18px;
    color: #0f172a;
}
.panel .block.svelte-1ipelgc {
    background: transparent;
}
.settings-stack > * + * {
    margin-top: 16px;
}
.chat-stack {
    display: flex;
    flex-direction: column;
    gap: 16px;
    flex: 1;
}
.chat-stack .gradio-container .component {
    background: transparent;
}
.record-row .wrap {
    flex: 1;
}
.send-btn {
    width: 100%;
    height: 54px;
    font-size: 16px;
    border-radius: 16px !important;
    background: linear-gradient(90deg, #6366f1, #8b5cf6);
    color: #fff;
}
.send-btn:hover {
    box-shadow: 0 18px 36px rgba(99, 102, 241, 0.25);
}
.status-box textarea {
    min-height: 90px !important;
}
"""


def main() -> None:
    with gr.Blocks(title="Zero-Shot Digital Human (Local LLM)", css=CUSTOM_CSS, theme=gr.themes.Soft()) as demo:
        with gr.Column(elem_classes=["hero-card"]):
            gr.Markdown("Proudly by **The Elite**", elem_classes=["the-elite"])
            gr.Markdown(
                "Zero-Shot Digital Human (æœ¬åœ°LLMç‰ˆæœ¬)",
                elem_classes=["hero-title"],
            )
            gr.Markdown(
                "å¯¹è¯å¼ç•Œé¢ï¼Œå®æ—¶å®Œæˆè¯­éŸ³é‡‡é›†ã€Whisper è½¬å†™ã€æœ¬åœ°LLMå›å¤ä¸ XTTS å…‹éš†éŸ³è‰²å›æ”¾ï¼Œè®©æœ¬åœ°æ•°å­—äººä½“éªŒæ›´å…·è´¨æ„Ÿã€‚"
                "<br>ä¸Šä¼ ç›®æ ‡éŸ³è‰²ï¼ŒæŒ‰ä½éº¦å…‹é£è®²è¯æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼Œå³å¯è·å¾—ä¸“å±éŸ³è‰²çš„å³æ—¶è¯­éŸ³åé¦ˆã€‚"
                "<br><strong>âš ï¸ è¯·ç¡®ä¿æœ¬åœ°LLMæœåŠ¡è¿è¡Œåœ¨ localhost:1234</strong>",
                elem_classes=["hero-subtitle"],
            )

        history_state = gr.State([])
        messages_state = gr.State([])

        with gr.Row(elem_classes=["main-row"]):
            with gr.Column(elem_classes=["panel"]):
                gr.Markdown("### è®¾ç½®é¢æ¿")
                with gr.Column(elem_classes=["settings-stack"]):
                    reference_input = gr.Audio(label="ç›®æ ‡éŸ³è‰² (wav)", type="filepath")
                    language_input = gr.Radio(
                        ["zh-cn", "en"],
                        value="zh-cn",
                        label="å›å¤è¯­è¨€",
                    )
                    status_box = gr.Textbox(
                        label="ç³»ç»Ÿæç¤º",
                        interactive=False,
                        placeholder="ä¸Šä¼ å‚è€ƒéŸ³è‰²åï¼Œå¼€å§‹å½•éŸ³å‘èµ·å¯¹è¯â€¦",
                        elem_classes=["status-box"],
                    )

            with gr.Column(elem_classes=["panel"]):
                gr.Markdown("### å¯¹è¯ç©ºé—´")
                with gr.Column(elem_classes=["chat-stack"]):
                    history_box = gr.Chatbot(label="Chat History", height=360)
                    reply_audio = gr.Audio(label="æœ€æ–°å…‹éš†è¯­éŸ³", interactive=False)
                    # éº¦å…‹é£è¾“å…¥ - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°ä»¥æ”¯æŒ Safari
                    mic_input = gr.Audio(
                        label="æŒ‰ä¸‹å¼€å§‹å½•éŸ³ (ç‚¹å‡»éº¦å…‹é£å›¾æ ‡)", 
                        sources=["microphone"], 
                        type="numpy",
                        streaming=False
                    )
                    # æ–‡ä»¶ä¸Šä¼ è¾“å…¥
                    file_input = gr.Audio(
                        label="æˆ–ä¸Šä¼ å½•å¥½çš„éŸ³é¢‘æ–‡ä»¶ (wavæ ¼å¼)",
                        sources=["upload"],
                        type="filepath"
                    )
                    submit_btn = gr.Button("å‘é€", variant="primary", elem_classes=["send-btn"])

        submit_btn.click(
            fn=chat_pipeline,
            inputs=[mic_input, file_input, reference_input, language_input, history_state, messages_state],
            outputs=[history_box, messages_state, reply_audio, status_box],
        )

    # ä½¿ç”¨ localhost ä»¥æ”¯æŒ Safari éº¦å…‹é£è®¿é—®
    # Safari è¦æ±‚ HTTPS æˆ– localhost æ‰èƒ½è®¿é—®éº¦å…‹é£
    print("\n" + "="*60)
    print("ğŸ¤– æœ¬åœ°LLMæ•°å­—äººç³»ç»Ÿ")
    print("="*60)
    print("ğŸ“‹ å¯åŠ¨æ£€æŸ¥æ¸…å•:")
    print("   âœ“ æœ¬åœ°LLMæœåŠ¡è¿è¡Œåœ¨: http://localhost:1234/v1")
    print("   âœ“ Webç•Œé¢å°†è¿è¡Œåœ¨: http://localhost:7860")
    print("")
    print("ğŸ¤ éº¦å…‹é£è®¿é—®æç¤º:")
    print("1. è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:7860")
    print("2. é¦–æ¬¡ä½¿ç”¨ä¼šå¼¹å‡ºéº¦å…‹é£æƒé™è¯·æ±‚ï¼Œè¯·ç‚¹å‡»'å…è®¸'")
    print("3. å¦‚é‡é—®é¢˜ï¼Œè¯·æ£€æŸ¥:")
    print("   - ç³»ç»Ÿè®¾ç½® > éšç§ä¸å®‰å…¨æ€§ > éº¦å…‹é£ > ç¡®ä¿æµè§ˆå™¨å·²å¼€å¯")
    print("   - æ¨èä½¿ç”¨ Chrome æµè§ˆå™¨ä»¥è·å¾—æ›´å¥½çš„å…¼å®¹æ€§")
    print("")
    print("ğŸ’¡ æœ¬åœ°LLMæœåŠ¡æ¨è:")
    print("   - LM Studio: https://lmstudio.ai/")
    print("   - Ollama (éœ€é…ç½®ä¸ºOpenAIå…¼å®¹æ¨¡å¼)")
    print("   - llama.cpp server")
    print("="*60 + "\n")
    
    demo.launch(
        server_name="127.0.0.1", 
        server_port=7860,
        share=False,
        inbrowser=True  # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    )


if __name__ == "__main__":
    main()
